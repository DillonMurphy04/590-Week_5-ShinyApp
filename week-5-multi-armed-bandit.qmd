---
title: "Adaptive Decision Analysis: FDR-Controlling Procedure and Multi-Armed Bandits"
author: "Dillon, Hannah, and Tyler"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    df-print: paged
    smooth-scroll: true
editor: visual
fontsize: 12pt
execute:
  warning: false
---

# Part 1: False Discovery Rate (FDR) Controlling Procedure

## Introduction / Background

When we perform many hypothesis tests, some will appear “significant” purely by chance, even if all null hypotheses are true. As the number of tests increases, so does the chance of getting at least one result that looks significant but isn’t actually real.

A false positive (or false discovery) happens when we reject the null hypothesis even though it’s actually true.

Traditional methods like the Bonferroni correction aim to control the probability of any false positives across all tests, the family-wise error rate (FWER) (the probability of getting at least one false positive). However, this approach can be overly conservative, greatly reducing statistical power. 

The Benjamini–Hochberg (BH) procedure (1995) takes a more flexible approach: it controls the expected proportion of false discoveries among all rejected hypotheses, the false discovery rate (FDR). This means controlling the expected proportion of false positives among all rejections.
This provides a better balance between detecting real effects and limiting false ones.

## 1.1 The Null Distribution of p-values

Under the null hypothesis, p-values follow a Uniform(0, 1) distribution.
The gray region in the plot shows the α = 0.05 rejection region.

```{r}
#| message: false
#| warning: false
# Load libraries
library(tidyverse)

# Parameters
mu0 <- 98.6
n <- 4
sigma <- 1
alpha0 <- 0.05

# Step 1: Simulate data under H0
n_sims <- 10000
samples <- replicate(n_sims, rnorm(n, mean = mu0, sd = sigma))

# Step 2: Compute p-values for each sample
sample_means <- colMeans(samples)
z_vals <- abs(sample_means - mu0) / (sigma / sqrt(n))
p_values <- 2 * pnorm(-z_vals)

# Step 3: Plot
tibble(p_value = p_values) |>
  ggplot(aes(p_value)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "skyblue", color = "white", alpha = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  geom_rect(aes(xmin = -Inf, xmax = alpha0, ymin = 0, ymax = Inf),
            fill = "gray", alpha = 0.01, inherit.aes = FALSE) +
  geom_vline(xintercept = alpha0, color = "orange", size = 1) +
  labs(
    title = "Distribution of p-values under H0",
    subtitle = "α = 0.05 (gray area = rejection region)",
    x = "p-value", y = "Density"
  ) +
  theme_minimal(base_size = 14)

```
## 1.2 The Alternative Distribution of p-values

When the alternative hypothesis is true (e.g., μ₁ = 97.5), p-values concentrate near 0 — showing strong evidence against H₀.

```{r}
#| message: false
#| warning: false
# Parameters
mu1 <- 97.5
n_sims <- 10000

# Simulate data under H1
samples <- replicate(n_sims, rnorm(n, mean = mu1, sd = sigma))

# Compute p-values (still testing against mu0)
sample_means <- colMeans(samples)
z_vals <- abs(sample_means - mu0) / (sigma / sqrt(n))
p_values <- 2 * pnorm(-z_vals)

# Plot
tibble(p_value = p_values) |>
  ggplot(aes(p_value)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "tomato", color = "white", alpha = 0.7) +
  geom_vline(xintercept = alpha0, color = "orange", size = 1) +
  geom_rect(aes(xmin = -Inf, xmax = alpha0, ymin = 0, ymax = Inf),
            fill = "gray", alpha = 0.05, inherit.aes = FALSE) +
  labs(
    title = expression("Distribution of p-values under " ~ H[1]),
    subtitle = "α = 0.05 (gray area = rejection region)",
    x = "p-value", y = "Density"
  ) +
  theme_minimal(base_size = 14)

```
## 1.3 Multiple Tests — No Correction

Now simulate m = 1000 independent tests, where 10% are true signals (H₁) and 90% are nulls (H₀).

```{r}
# Parameters
m <- 1000
p1 <- 0.10
set.seed(123)

# Simulate true/false hypotheses
I_vec <- rbinom(m, size = 1, prob = p1)
mu_vec <- if_else(I_vec == 0, mu0, mu1)

# Generate data and compute p-values
X_mat <- matrix(rnorm(m * n, mean = rep(mu_vec, each = n), sd = sigma), nrow = m, byrow = TRUE)
xbar <- rowMeans(X_mat)
z <- abs(xbar - mu0) / (sigma / sqrt(n))
pvals <- 2 * pnorm(-z)

# Plot
sim_df <- tibble(I = I_vec, p_value = pvals)
R <- sum(pvals <= alpha0)

ggplot(sim_df, aes(p_value)) +
  geom_histogram(bins = 20, boundary = 0) +
  geom_vline(xintercept = alpha0, color = "orange", linewidth = 1) +
  labs(
    title = "Simulated p-values under mixture of H0 and H1",
    subtitle = paste0("α cutoff = 0.05, number of rejections = ", R),
    x = "p-value", y = "Count"
  ) +
  theme_minimal()
```
## 1.4 Benjamini–Hochberg (BH) Procedure

The BH procedure controls FDR at level q = 0.10.
It sorts p-values and finds the largest p₍ᵢ₎ ≤ (i/m)q — defining a data-adaptive cutoff.

```{r}
# Parameters
q <- 0.10

# Rank p-values and apply BH
bh_tbl <- tibble(pvalue = pvals) |>
  arrange(pvalue) |>
  mutate(rank = row_number(),
         thresh = (rank / m) * q,
         keep = pvalue <= thresh)

k_max <- if (any(bh_tbl$keep)) max(which(bh_tbl$keep)) else 0
t_BH <- if (k_max > 0) bh_tbl$pvalue[k_max] else 0

# Rejections
res_df <- tibble(pvalue = pvals, I = I_vec, reject = pvals <= t_BH)
R <- sum(res_df$reject)
V <- sum(res_df$reject & res_df$I == 0)
S <- sum(res_df$reject & res_df$I == 1)
m0 <- sum(res_df$I == 0)
m1 <- sum(res_df$I == 1)
FDP <- if (R > 0) V / R else 0
Power <- if (m1 > 0) S / m1 else NA_real_

tibble(m, q, m0, m1, R, S, V, FDP, Power, t_BH)

# Plot
ggplot(res_df, aes(pvalue)) +
  geom_histogram(bins = 20, boundary = 0) +
  geom_vline(xintercept = t_BH, color = "orange", linewidth = 1) +
  labs(
    title = "P-values with BH cutoff",
    subtitle = paste0("q = 0.10, cutoff t_BH = ", signif(t_BH, 3),
                      ", rejections R = ", R),
    x = "p-value", y = "Count"
  ) +
  theme_minimal()
```
## 1.5 Comparing BH to Bonferroni

Finally, compare Bonferroni and BH rejections.
Bonferroni controls FWER (probability of any false positives), while BH controls FDR (expected proportion of false discoveries).

```{r}
# Compute rejections
t_Bonf <- alpha0 / m
R_Bonf <- sum(pvals <= t_Bonf)
R_BH   <- sum(pvals <= t_BH)

# Create histogram
ggplot(tibble(p_value = pvals), aes(p_value)) +
  geom_histogram(bins = 40, boundary = 0, fill = "gray70", color = "white") +

  # Add vertical lines for cutoffs
  geom_vline(xintercept = t_Bonf, color = "red", linewidth = 1) +
  geom_vline(xintercept = t_BH, color = "orange", linewidth = 1) +
  
  # Axis limits to zoom in near the small p-value region
  coord_cartesian(xlim = c(0, max(t_BH * 3, 0.01))) +
  
  labs(
    title = "Zoomed-In Comparison of Bonferroni vs BH Cutoffs",
    subtitle = paste0(
      "Bonferroni (red): α/m = ", signif(t_Bonf, 3), " → R = ", R_Bonf, "\n",
      "BH (orange): q = 0.10 → t_BH = ", signif(t_BH, 3), " → R = ", R_BH
    ),
    x = "p-value (zoomed in near 0)", 
    y = "Count",
    caption = "Note: Bonferroni controls the Family-Wise Error Rate (FWER); BH controls the False Discovery Rate (FDR)."
  ) +
  theme_minimal(base_size = 14)

```
## Reflect and Discuss!

How does the Bonferroni correction’s goal of controlling the family-wise error rate (FWER) differ from the Benjamini–Hochberg (BH) procedure’s goal of controlling the false discovery rate (FDR)?

Can you think of other contexts where a researcher may prefer to use one type of error control that is more appropriate than the other?

Looking at the simulated plots, what do you notice about how many hypotheses each method rejects?

# Part 2: Bayesian Optimization

## Activity Objective

The multi-armed bandit problem captures the tradeoff between exploring new options and exploiting the best one found so far — like choosing which slot machine to play when their payouts are uncertain. In this activity, we’ll connect these ideas by visualizing how Bayesian Optimization uses uncertainty and observed results to make adaptive, data-driven decisions — just like a clever gambler learning which arm to pull next.

> To start run the Shiny app `app.R`. Or go to https://izsl0n-dillonmurphy04.shinyapps.io/Multi-Armed-Bandit/

# 2: The Multi-Armed Bandit Problem

## 2.1 Formal setup

-   We have $K$ arms (options), indexed $k=1,\dots,K$.
-   Each arm $k$ yields a binary reward $R_{k}\in\{0,1\}$ with **unknown** success probability $p_k\in(0,1)$.
-   At round $t=1,2,\dots,T$, we **choose** an arm $A_t\in\{1,\dots,K\}$, observe reward $R_{A_t,t}$, and update our strategy.

**Objective.** Maximize **cumulative reward** or equivalently **minimize regret** relative to the best arm $$
p_*=\max_{k}p_k,\qquad
k_*\in\operatorname*{arg\,max}_{k}p_k.
$$

-   **Cumulative reward** after $T$ rounds: $$
    S_T=\sum_{t=1}^{T}R_{A_t,t}.
    $$

-   **Cumulative regret** after $T$ rounds: $$
    \mathcal{R}(T)=T\,p_*-\sum_{t=1}^{T}R_{A_t,t}.
    $$

**Questions (before opening the app).** - If you knew $p_k$ for all $k$, what would you do at each round? - If you *don’t* know $p_k$, why can a purely exploitative strategy be risky? - Propose a way to quantify how “costly” learning is.

## 2.2 Exploration vs. exploitation

-   **Exploration**: try arms with uncertain payoffs to learn $p_k$.
-   **Exploitation**: pull the arm that currently looks best to earn reward.
-   **Tension**: explore enough to identify $k_*$, but not so much that you waste pulls.

**Questions.** - Give an example where early exploration clearly pays off later. - What observable signals suggest “it’s time to exploit”?

# 3. Single-Arm Learning (Learn tab in the app)

This tab illustrates **Bayesian learning** for *one* Bernoulli arm.

## 3.1 Model and posterior

-   **Likelihood.** If we observe $s$ successes and $f$ failures from one arm, $$
    R_1,\dots,R_n\mid p\sim\operatorname{Bernoulli}(p),\qquad n=s+f.
    $$

-   **Conjugate prior.** $p\sim\operatorname{Beta}(\alpha_0,\beta_0)$.

-   **Posterior.** $$
    p\mid\text{data}\sim\operatorname{Beta}(\alpha_0+s,\;\beta_0+f).
    $$

-   **Posterior mean** and a $100(1-\gamma)\%$ **credible interval**: $$
    \mathbb{E}[p\mid\text{data}]
    =\frac{\alpha_0+s}{\alpha_0+\beta_0+s+f},
    \qquad
    \text{CI}_{1-\gamma}=[\,q_{\gamma/2},\;q_{1-\gamma/2}\,],
    $$ where $q_q$ is the $q$-quantile of $\operatorname{Beta}(\alpha_0+s,\beta_0+f)$.

## 3.2 What the app shows

-   **Posterior Beta curve**: your updated belief over $p$.
-   **Table**: prior hyperparameters, observed counts $(s,f)$, posterior hyperparameters, posterior mean, and (optionally) the *true* $p$ (hidden by default).
-   In the app, a **“success”** is a realized reward of $1$; a **“failure”** is $0$.

## 3.3 Activity A — Single-arm Bayesian updating

1.  Open **Learn (1-Arm)**. Set prior $\alpha_0=\beta_0=1$ and **hide** the true $p$.
2.  Click **Pull the Arm** 10 times (with “Pulls per click” set to 1). Record $s$, $f$, and the posterior mean.
3.  Reveal the true $p$. Did your $95\%$ credible interval contain it?
4.  Continue pulling until total pulls $n\ge 50$. Observe how the posterior narrows.

**Questions.** - How do larger prior values $(\alpha_0,\beta_0)$ change early-stage behavior? - If the prior strongly favors high $p$ but the data are unfavorable, how quickly does the posterior adjust? - Why is conjugacy (Beta–Bernoulli) useful here?

**What to learn from this activity.** - Posterior uncertainty **shrinks** with data. - The posterior mean is a smoothed estimate (data + prior). - This exact updating mechanism underlies **Thompson Sampling** in multi-arm settings.

# 4. Multi-Arm Policies (Play tab)

This tab lets you **run a bandit** with $K$ arms and compare policies.

## 4.1 Interpreting the visuals

-   **Cumulative Regret**: the curve of $\mathcal{R}(t)$ versus $t$. *Lower is better.* A flatter slope means faster learning/focus on near-optimal arms.
-   **Pulls by Arm**: bar chart with counts $n_k(t)$ for each arm; in good runs the best arm’s bar grows dominant.
-   **Success vs Failure (stacked bars)**: per arm, shows **Hits** (successes) and **Misses** (failures) so you can see outcome balance and where exploration went.
-   **Per-arm Summary table**: for each arm shows **Pulls**, **Hits**, **Misses**, **Empirical mean** (hits/pulls), **Posterior mean** ($\alpha_k/(\alpha_k+\beta_k)$), **% of pulls**, and (optionally) **True** $p$ when revealed.

## 4.2 Policies

### (a) $\varepsilon$-Greedy

-   Maintain sample means $\hat{\mu}_k(t)$.
-   With probability $\varepsilon$, **explore** uniformly; with probability $1-\varepsilon$, **exploit** the arm $\arg\max_k\hat{\mu}_k(t)$.
-   Pros: simple baseline.\
-   Cons: fixed exploration may be too much late, too little early.

### (b) UCB1 (Upper Confidence Bound)

Choose the arm that maximizes an **optimistic** estimate: $$
\operatorname{UCB1}_k(t)=\hat{\mu}_k(t)+\sqrt{\frac{2\ln t}{n_k(t)}},
$$ where $n_k(t)$ is the number of times arm $k$ has been pulled up to round $t$. The confidence bonus **shrinks** as $n_k(t)$ grows.

-   Pros: strong regret guarantees in iid Bernoulli settings.\
-   Cons: can be conservative or mis-calibrated outside assumptions.

### (c) Thompson Sampling (TS)

Maintain a **Beta posterior** per arm: $$
p_k\mid\text{data}\sim\operatorname{Beta}(\alpha_k,\beta_k),\qquad
\alpha_k\leftarrow\alpha_k+\text{successes},\;\beta_k\leftarrow\beta_k+\text{failures}.
$$ Each round: 1. Sample $\tilde{p}_k\sim\operatorname{Beta}(\alpha_k,\beta_k)$ independently for each arm.\
2. Pull $\arg\max_k\tilde{p}_k$.

-   Pros: elegant Bayesian decision rule; excellent empirical performance; “probability matching”.\
-   Cons: requires a model and priors; early behavior can be prior-sensitive.

## 4.3 Activity B — Explore vs. exploit in practice

1.  In **Play (Multi-Arm)**, set $K=4$, **Randomize arms**, Horizon $=100$. After simulation go to the Replay tab and build a replay to watch how the arms were chosen.
2.  Run $\varepsilon$-Greedy with $\varepsilon=0.30$ and then with $\varepsilon=0.05$. Compare **Cumulative Regret**, **Pulls by Arm**, and **Success vs Failure**.
3.  On the same instance (click **Reset** once to fix a new draw), run **UCB1** and **TS**. Inspect the **Per-arm Summary** (empirical vs posterior mean; % pulls).

**Questions.** - Which policy locks onto the best arm fastest? What do you see in **Pulls by Arm** and **Success vs Failure**? - How does increasing $\varepsilon$ change early vs late performance? - Do posterior means typically shrink toward empirical means as pulls grow? Why?

**What to learn from this activity.** - $\varepsilon$-Greedy uses a fixed exploration dial; performance depends on $\varepsilon$. - UCB1 explores where uncertainty remains (optimism). - TS balances explore/exploit by sampling from posterior beliefs. - Per-arm summaries provide quick diagnostics of allocation quality.

# 5. Replicated Evaluation (Compare tab)

Single runs can be noisy. Replicates provide a **fairer comparison**.

## 5.1 What the tab computes

For a fixed horizon $H$ and arm configuration $\{p_k\}$, it runs each policy multiple times and records **terminal regret** $\mathcal{R}(H)$, then produces: - A **mean** $\pm$ 95% CI plot per policy; and - A **per-policy summary table** (not per-run) with: $n$, mean, sd, median, 2.5% and 97.5% quantiles of $\mathcal{R}(H)$.

## 5.2 Activity C — Compare policies with replication

1.  In **Compare**, set $K=4$, Horizon $=300$, Replicates $=100$, $\varepsilon=0.1$. Click **Run Comparison**.
2.  Inspect the **mean** $\pm$ 95% CI plot and the **per-policy summary** table.
3.  Turn off randomization and set arms close together (e.g., $p=[0.56,0.54,0.53,0.50]$). Re-run.

**Questions.** - Which policy has the **lowest mean terminal regret**? Are differences practically meaningful given the CIs and quantiles? - How do the summaries change when arms are harder to distinguish (smaller gaps)? - What happens to regret and pull allocation as $K$ increases?

**What to learn from this activity.** - Harder problems (small gaps) increase regret for **every** policy. - TS/UCB1 typically dominate $\varepsilon$-Greedy baselines. - Replication reduces variance and supports stronger conclusions.
